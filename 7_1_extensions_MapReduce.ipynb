{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWSniNPAjjQh"
      },
      "source": [
        "<p><img alt=\"udeA logo\" height=\"120px\" src=\"https://github.com/freddyduitama/images/blob/master/logo.png?raw=true\" align=\"left\" hspace=\"10px\" vspace=\"0px\" style=\"width:107px;height:152px;\"></p>\n",
        "\n",
        "# <center> <font color='0B5345'> Extensions of MapReduce: SPARK </font> </center>\n",
        "\n",
        "<center><font face=\"Verdana\"><a  href=\"https://colab.research.google.com/drive/108ndSjtafWCtZMS3W_VLXh34gMHIegh4#scrollTo=EWSniNPAjjQh\">The Alignment problem.</a> <a>&nbsp;&nbsp; | </a><a  href=\"https://colab.research.google.com/drive/1SgcWf2Z6jSYh5qcxOZL-QU6QZhpY87Nj#scrollTo=wUijAhrCm0-h\" >TOC</a><a>&nbsp;&nbsp;    |</a> <a>&nbsp;&nbsp;</a> <a   href=\"https://colab.research.google.com/drive/13zDbsSAVDTwVtO9LN-Zj47QTr6FWulk1#scrollTo=5J7bfvBqcMGT\">Introduction to Spark.</a></font><center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im-rLP7aPM5p"
      },
      "source": [
        "## 1. Introduction.  \n",
        "<font face=\" Book Antiqua\" size=\"2\">\n",
        "<p align=\"justify\">\n",
        "MapReduce proved so influential that it spawned a number of extensions and modifications. These systems typically share a number of characteristics with MapReduce systems:\n",
        "<ol>\n",
        "<li>They are built on a distributed file system. </li>\n",
        "<li>They manage very large numbers of tasks that are instantiations of a small number of user-written functions. </li>\n",
        "<li>They incorporate a method for dealing with most of the failures that occur during the execution of a large job, without having to restart that job from the beginning.</li>\n",
        "<li>Example is SPARK. </li>\n",
        "</ol>\n",
        "    \n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpHxVLvmPM5q"
      },
      "source": [
        "## 2. SPARK.  <br>\n",
        "<font face=\"Book Antiqua\"  size=\"2\">\n",
        "<p align=\"justify\">\n",
        "Spark is a <em>“computational engine”</em> that is responsible for scheduling, distributing and monitoring applications consisting of many computational tasks across many worker machines or a computing cluster. <br>\n",
        "Spark extends the mapReduce model for complex jobs and it has proven to be more efficient for batch applications, iterative algorithms, interactive queries, streaming  and online processing. It was created at the University of California, Berkeley, in 2009. <br>\n",
        "<center><img src=\"https://github.com/freddyduitama/images/blob/master/spark-framework.png?raw=true\"  height=\"200\" width=\"400\"></center>\n",
        "<caption><center><font color='0B5345'> <u> <b>Figure 1:</b><br> </u>Spark Tools</font></center></caption>\n",
        "\n",
        "<font face=\"Book Antiqua\"  size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<b>2.1. Data abstraction.</b><br>\n",
        "SPARK supports three types of data abstractions.\n",
        "<ul align=\"justify\">\n",
        "<li><b><em>Resilient Distributed Dataset (RDD).</em></b> <br>\n",
        "An RDD is an <font color='0B5345'><b>immutable</b></font> distributed collection of objects, which is fault-tolerant. <br>\n",
        "An RDD is normally broken into fragments that may be held at different compute nodes. The Distributed File Systems keeps several RDD replicas; in this way,  they are “resilient” in the sense that we expect to be able to recover from the loss of any or all fragments of an RDD. Unlike the key-value-pair abstraction of MapReduce, there is no restriction on the type of the elements that comprise an RDD. </li>\n",
        "<li><b><em>Datasets: </em></b> <br>\n",
        "They are a distributed and <font color='0B5345'><b>immutable</b></font> table-like collection  with well-defined rows and columns. They are especially attractive for writing large applications, with multiple software modules which must interact through well-defined interfaces. For example, a Dataset[Person] will be guaranteed to contain objects of class Person.\n",
        "</li>\n",
        "<li><b>Dataframes: </b><br>\n",
        "A Dataframe is a distributed and <font color='0B5345'><b>immutable</b></font> collection of rows organized into named columns. It is equivalent to a table in relational DB or DataFrames structure in R and Python. A DataFrame can be constructed from data files, relational tables, external databases or existing RDDs. It shares common characteristics with RDD: Immutable, and Distributed. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgTue1yVPlCi"
      },
      "source": [
        "<font face=\"Book Antiqua\"  size=\"3\">\n",
        "<p align=\"justify\">\n",
        "<b>2.2. Computational model.</b><br>\n",
        "A Spark applications has a <em>driver program</em> that launches various parallel operations called <em>executors</em> on a cluster (figure 2). Every <em>dataset/RDD/dataframe</em> has a fixed number of partitions that determine the degree of parallelism to use when executing operations on the RDD. The parallelism  degree can be customized by the programmer. <br>\n",
        "A Spark job  supports two types of operations: <em>transformations</em>, which create a new <em>RDD/dataframe</em> from an existing one, and <em>actions</em>, which return a value to the driver program after running a computation on the dataset.\n",
        "<center> <img src=\"https://github.com/freddyduitama/images/blob/master/lineage-graph.png?raw=true\"  height=\"150\" width=\"450\"> <img src=\"https://github.com/freddyduitama/images/blob/master/spark-application.png?raw=true\"  height=\"250\" width=\"250\"></center>\n",
        "<caption><center><font color='0B5345'> <u> <b>Figure 2:</b><br> </u>Lineage graph and computational model.</font></center></caption>\n",
        "<font face=\"Book Antiqua\"  size=\"3\">\n",
        "<p align=\"justify\">\n",
        "<b>2.3. SPARK program lifecycle.</b><br>\n",
        "<ol align=\"justify\">\n",
        "<li>The <b>driver program</b> contains your application’s main function and defines distributed <em>datasets/RDDs/dataframes</em> on the cluster as INPUT. </li>\n",
        "<li>Apply <b>transformations</b> to an RDD/Dataframe/Dataset.  <em>Transformations</em> (operations like map/filter/ flatMap /distinct / reduceByKey)</li>\n",
        "<li>Apply <b>actions</b> to an RDD/Dataframe/Dataset.\n",
        "<em>(operations like reduce/collect/count/SaveAsTextFile)</em> </li>\n",
        "</ol>\n",
        "<font face=\"Book Antiqua\"  size=\"3\">\n",
        "<p align=\"justify\">\n",
        "<font color='0B5345'><b>Lineage graph:</b></font>\n",
        "The SPARK program is represented as a set of Directed Acyclic Graphs (DAG). i.e. a track of the set of dependencies between different RDDs/Dataframe/dataset (figure 2). This set of DAGs can be used to compute each step on  demand and to recover lost data.\n",
        "\n",
        "<font color='0B5345'><b>Lazy evaluation</b></font><br>\n",
        "Transformed RDDs/Datasets/Dataframes are computed only when programmers use them in an action (lazily). It means that Spark will not begin to execute until it sees an action. <br> <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH-uyzAtQbT8"
      },
      "source": [
        "## 3. SPARK application.\n",
        "<font face=\"Book Antiqua\"  size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<img src=\"https://github.com/freddyduitama/images/blob/master/spark-application1.png?raw=true\" align=\"left\" height=\"400\" width=\"500\">\n",
        "<ul align=\"justify\">\n",
        "<li>Each <em>SPARK application</em> may contain many jobs. See figure 3.</li>\n",
        "<li> A <em>job</em>  creates new RDDs or <em>transforms</em> existing data abstraction (RDDs/Datasets/Dataframes) and calls <em>actions</em> on them to compute a result. It corresponds to several transformations ending with one action (one Directed Acyclic Graph DAG). See figure 2. <br>\n",
        "   </li>\n",
        "<li>The Directed Acyclic Graph (a DAG) is a graph  with  several stages. </li>\n",
        "<li>Each <em>stage</em> consists of a collection of <em>tasks</em> that are performed on the executors <b>without involving the driver</b>; i.e. tasks represent local computation. See right side in figure 3. </li>\n",
        "<li>The transition of one stage to another corresponds to one <em>wide dependency</em> which produces a shuffle operation. </li>\n",
        "<li><em>The shuffle operation</em> is a Spark’s mechanism for redistributing data so that it’s grouped differently across partitions. This typically involves exchanging data across executors and machines, making the shuffle a complex and costly operation. Shuffle also generates a large number of intermediate files on disk.</li>\n",
        "</ul>\n",
        "\n",
        "<caption><font color='0B5345' align=\"left\"> &emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;<u> <b>Figure 3:</b><br> </u> &emsp; &emsp;&emsp; &emsp;&emsp; &emsp;&emsp; &emsp;SPARK application.</font></caption><br>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avBkpsfiPM5t"
      },
      "source": [
        "## 4. Example.\n",
        "<font face=\"Book Antiqua\"  size=\"2\">\n",
        "<p align=\"justify\">\n",
        "<img src=\"https://github.com/freddyduitama/images/blob/master/spark-application.png?raw=true\"  align=\"left\" height=\"250\" width=\"350\">\n",
        "The  code in example uses the <em>reduceByKey</em> operation on <em>key-value</em> pairs to count how many times each line of text occurs in the input file. <font face=\"Book Antiqua\"  size=\"2\"><a href=\"https://colab.research.google.com/drive/1jlCjdK1uAKmFwk1L0OQYwoHQ_91Iedow#scrollTo=efQG-m4IV7sh\">to run the code</a> </font><br>\n",
        " <font color='992208'><b>lines</b></font>=  sc.textFile(\"data.txt\")  <br>\n",
        " <font color='992208'><b>pairs</b></font>= lines.map(<b>lambda</b> s: (s, 1))<br>\n",
        "<font color='992208'><b> counts </b></font>= pairs.reduceByKey(<b>lambda</b> a, b: a + b) <br>\n",
        "<font color='992208'><b> counts</b></font>.collect()<br><br>\n",
        "<b><font color='0B5345'>Compact version: </font></b><br>\n",
        "<font color='992208'><b>sc.textFile</font></b>('data.txt').<font color='992208'><b>map</font></b>(lambda s: (s, 1)).<font color='992208'><b>reduceByKey</font></b>(lambda a, b: a + b).<font color='992208'><b>collect</font></b>()<br><br>\n",
        "<right><caption><font color='0B5345' align=\"left\"> &emsp;&emsp;&emsp;<u><b>Example 1:</b></u> SPARK code </font></caption></right>\n",
        "\n",
        " <font color='0B5345'><b>Steps:</b></font>   \n",
        "    <ol>\n",
        "<li>By cause of the collect() action, a Spark driver launches one executor for each partition of the data file data.txt </li>\n",
        "<li>SPARK driver creates a distributed RDD from  the file data.txt  This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. </li>\n",
        "        <li>For each executor  </li>\n",
        "<ul>\n",
        "<li>INPUT: SPARK driver assigns one RDD partition $\\to$  [\"line1\", \"line2\",..., $line_k$].</li>\n",
        "<li>MAP FUNCTION: for each line into the file partition the map function produce the pair $(s,1)$ <br>\n",
        "    <em>Combiner: </em>the executor runs the reduceBykey function.</li>\n",
        "<li>OUTPUT {$(s, value_i)$}  where $value_i$ is how many times each line of text occurs in  the partition. <br>The executor sends this pair to the driver. (shuffle operation)</li>\n",
        "</ul>\n",
        "<li>The driver process receives a set of pairs {$(s,value_i)$} from each executor. It run the reduce function again and obtain how many times each line of text occurs in the entire file, the pair $(s,value)$</li>\n",
        "</ol>    \n",
        "    \n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkNa_Tz8PM5u"
      },
      "source": [
        "## Video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGRakHE7PM5v",
        "outputId": "e6411a02-564e-4ddd-f5ed-cab816fec25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "vid = YouTubeVideo(\"PGEmWQ9HELg\",width=200, height=150,)\n",
        "display(vid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"200\"\n",
              "            height=\"150\"\n",
              "            src=\"https://www.youtube.com/embed/PGEmWQ9HELg\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fb9c13372d0>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn87LMr2ooZC"
      },
      "source": [
        "## Bibliography and related resources.\n",
        "\n",
        "<p align=\"justify\"><font face=\"Verdana\" size=\"2.5\">[1]&nbsp;&nbsp;&nbsp;&nbsp;Matei Zaharia, et. al. <cite>Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing.</cite> NSDI’12 Proceedings of the 9th <br>&emsp;&emsp;&emsp;USENIX conference on Networked Systems Design and Implementation. USA, San Jose, CA. 2012. pp. 15-28.<a href=\"https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\"> here</a>  </p>\n",
        "\n",
        "<p align=\"justify\"><font face=\"Verdana\" size=\"2.5\"> [2]&nbsp;&nbsp;&nbsp;&nbsp;Chambers Bill & Zaharia Matei. <cite>SPARK The definitive Guide. Big Data processing made simple.</cite> O’Reilly Media Inc.  USA. 2018. </p>\n",
        "\n",
        "<p align=\"justify\"><font face=\"Verdana\" size=\"2.5\">[3]&nbsp;&nbsp;&nbsp;&nbsp;Karau, H and Warren, R. <cite> High Performance SPARK. </cite>O’Reilly Media, Inc. USA. 2017.</p>\n",
        "\n",
        "<p align=\"justify\"><font face=\"Verdana\" size=\"2.5\">[3]&nbsp;&nbsp;&nbsp;&nbsp;Apache SPARK <cite> Unified engine for large-scale data analytics </cite> USA. 2018. <a href=\"https://spark.apache.org/\"> here</a>  </p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWkNSGVJvNE3"
      },
      "source": [
        "<p align=\"left\"><b><font face='Courier New' color=\"black\" align=\"left\" size=4>Copyright.</font></b>\n",
        "<img alt=\"udeA logo\" height=\"120px\" src=\"https://github.com/freddyduitama/images/blob/master/in2lab.png?raw=true\" align=\"right\" hspace=\"10px\" vspace=\"0px\" height=\"120\" width=\"350\"\">\n",
        "                                                                                                                              \n",
        "<font face='Verdana' size=2>\n",
        "John Freddy Duitama M.<br>\n",
        "Universidad de Antioquia.<br>\n",
        "Apartado Aéreo 1226 | Dirección: calle 67 No. 53 - 108.<br>\n",
        "Medellín, Colombia. South America.\n",
        "    \n",
        "</p>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3GBgqKNPM5z"
      },
      "source": [
        "<center><b><font color='0B5345' face=\"Lucida Calligraphy,Comic Sans MS,Lucida Console\" size=\"4\">Universidad de Antioquia.</font></b> </center>"
      ]
    }
  ]
}